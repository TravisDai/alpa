import copy
import torch


def apply_to_sample(f, sample):
    if hasattr(sample, "__len__") and len(sample) == 0:
        return {}

    def _apply(x):
        if torch.is_tensor(x):
            return f(x)
        elif isinstance(x, dict):
            return {key: _apply(value) for key, value in x.items()}
        elif isinstance(x, list):
            return [_apply(x) for x in x]
        elif isinstance(x, tuple):
            return tuple(_apply(x) for x in x)
        elif isinstance(x, set):
            return {_apply(x) for x in x}
        else:
            return x

    return _apply(sample)


def move_to_cuda(sample, device=None):
    device = device or torch.cuda.current_device()

    def _move_to_cuda(tensor):
        # non_blocking is ignored if tensor is not pinned, so we can always set
        # to True (see github.com/PyTorchLightning/pytorch-lightning/issues/620)
        return tensor.to(device=device, non_blocking=True)

    return apply_to_sample(_move_to_cuda, sample)


# def resolve_max_positions(*args):
#     """Resolve max position constraints from multiple sources."""
#
#     def map_value_update(d1, d2):
#         updated_value = copy.deepcopy(d1)
#         for key in d2:
#             if key not in updated_value:
#                 updated_value[key] = d2[key]
#             else:
#                 updated_value[key] = min(d1[key], d2[key])
#         return updated_value
#
#     def nullsafe_min(l):
#         minim = None
#         for item in l:
#             if minim is None:
#                 minim = item
#             elif item is not None and item < minim:
#                 minim = item
#         return minim
#
#     max_positions = None
#     for arg in args:
#         if max_positions is None:
#             max_positions = arg
#         elif arg is not None:
#             max_positions, arg = _match_types(max_positions, arg)
#             if isinstance(arg, float) or isinstance(arg, int):
#                 max_positions = min(max_positions, arg)
#             elif isinstance(arg, dict):
#                 max_positions = map_value_update(max_positions, arg)
#             else:
#                 max_positions = tuple(map(nullsafe_min, zip(max_positions, arg)))
#
#     return max_positions